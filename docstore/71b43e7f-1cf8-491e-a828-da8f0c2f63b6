1.1 Overﬁtting

But why does it matter? Surely we could just increase the number of hidden lay- ers in our network, and perhaps increase the number of neurons within them? The simple answer to this question is no. This is down to two reasons, one be- ing the simple problem of not having unlimited computational power and time to train these huge ANNs.

The second reason is stopping or reducing the effects of overﬁtting. Overﬁtting is basically when a network is unable to learn effectively due to a number of reasons. It is an important concept of most, if not all machine learning algo- rithms and it is important that every precaution is taken as to reduce its effects. If our models were to exhibit signs of overﬁtting then we may see a reduced ability to pinpoint generalised features for not only our training dataset, but also our test and prediction sets.

This is the main reason behind reducing the complexity of our ANNs. The less parameters required to train, the less likely the network will overﬁt - and of course, improve the predictive performance of the model.

2 CNN architecture

As noted earlier, CNNs primarily focus on the basis that the input will be com- prised of images. This focuses the architecture to be set up in way to best suit the need for dealing with the speciﬁc type of data.

4 Keiron O’Shea et al.

One of the key differences is that the neurons that the layers within the CNN are comprised of neurons organised into three dimensions, the spatial dimen- sionality of the input (height and the width) and the depth. The depth does not refer to the total number of layers within the ANN, but the third dimension of a activation volume. Unlike standard ANNS, the neurons within any given layer will only connect to a small region of the layer preceding it.

In practice this would mean that for the example given earlier, the input ’vol- ume’ will have a dimensionality of 64 × 64 × 3 (height, width and depth), lead- ing to a ﬁnal output layer comprised of a dimensionality of 1 × 1 × n (where n represents the possible number of classes) as we would have condensed the full input dimensionality into a smaller volume of class scores ﬁled across the depth dimension.
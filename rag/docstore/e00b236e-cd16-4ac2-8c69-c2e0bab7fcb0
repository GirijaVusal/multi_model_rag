Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks

Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun

6 1 0 2 n a J 6 ] V C . s c [ 3 v 7

Abstract—State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features—using the recently popular terminology of neural networks with “attention” mechanisms, the RPN component tells the uniﬁed network where to look. For the very deep VGG-16 model [3], our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.

Index Terms—Object Detection, Region Proposal, Convolutional Neural Network.

1 INTRODUCTION

One may note that fast region-based CNNs take advantage of GPUs, while the region proposal meth- ods used in research are implemented on the CPU, making such runtime comparisons inequitable. An ob- vious way to accelerate proposal computation is to re- implement it for the GPU. This may be an effective en- gineering solution, but re-implementation ignores the down-stream detection network and therefore misses important opportunities for sharing computation.

Recent advances in object detection are driven by the success of region proposal methods (e.g., [4]) and region-based convolutional neural networks (R- CNNs) [5]. Although region-based CNNs were com- putationally expensive as originally developed in [5], their cost has been drastically reduced thanks to shar- ing convolutions across proposals [1], [2]. The latest incarnation, Fast R-CNN [2], achieves near real-time rates using very deep networks [3], when ignoring the time spent on region proposals. Now, proposals are the test-time computational bottleneck in state-of-the-art detection systems.

9

4

1

i)

0

.

6

0

In this paper, we show that an algorithmic change— computing proposals with a deep convolutional neu- ral network—leads to an elegant and effective solution where proposal computation is nearly cost-free given the detection network’s computation. To this end, we introduce novel Region Proposal Networks (RPNs) that share convolutional layers with state-of-the-art object detection networks [1], [2]. By sharing convolutions at test-time, the marginal cost for computing proposals is small (e.g., 10ms per image).

5

1

:

v

i,

i

X

Region proposal methods typically rely on inex-

r

pensive features and economical inference schemes. Selective Search [4], one of the most popular meth- ods, greedily merges superpixels based on engineered low-level features. Yet when compared to efﬁcient detection networks [2], Selective Search is an order of magnitude slower, at 2 seconds per image in a CPU implementation. EdgeBoxes [6] currently provides the best tradeoff between proposal quality and speed, at 0.2 seconds per image. Nevertheless, the region proposal step still consumes as much running time as the detection network.

a

Our observation is that the convolutional feature maps used by region-based detectors, like Fast R- CNN, can also be used for generating region pro- posals. On top of these convolutional features, we construct an RPN by adding a few additional con- volutional layers that simultaneously regress region bounds and objectness scores at each location on a regular grid. The RPN is thus a kind of fully convo- lutional network (FCN) [7] and can be trained end-to- end speciﬁcally for the task for generating detection proposals.

• S. Ren is with University of Science and Technology of China, Hefei, China. This work was done when S. Ren was an intern at Microsoft Research. Email: sqren@mail.ustc.edu.cn

• K. He and J. Sun are with Visual Computing Group, Microsoft Research. E-mail: {kahe,jiansun}@microsoft.com

RPNs are designed to efﬁciently predict region pro- posals with a wide range of scales and aspect ratios. In contrast to prevalent methods [8], [9], [1], [2] that use

• R. Girshick is with Facebook AI Research. The majority of this work was done when R. Girshick was with Microsoft Research. E-mail: rbg@fb.com

1

multiple filter sizes

Ws : s_:_/’ multiple references

feature map

Figure 1: Different schemes for addressing multiple scales and sizes. (a) Pyramids of images and feature maps are built, and the classiﬁer is run at all scales. (b) Pyramids of ﬁlters with multiple scales/sizes are run on the feature map. (c) We use pyramids of reference boxes in the regression functions.

pyramids of images (Figure 1, a) or pyramids of ﬁlters (Figure 1, b), we introduce novel “anchor” boxes that serve as references at multiple scales and aspect ratios. Our scheme can be thought of as a pyramid of regression references (Figure 1, c), which avoids enumerating images or ﬁlters of multiple scales or aspect ratios. This model performs well when trained and tested using single-scale images and thus beneﬁts running speed.

To unify RPNs with Fast R-CNN [2] object detec- tion networks, we propose a training scheme that alternates between ﬁne-tuning for the region proposal task and then ﬁne-tuning for object detection, while keeping the proposals ﬁxed. This scheme converges quickly and produces a uniﬁed network with convo- lutional features that are shared between both tasks.1

mercial systems such as at Pinterests [17], with user engagement improvements reported.

In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the basis of several 1st-place entries [18] in the tracks of ImageNet detection, Ima- geNet localization, COCO detection, and COCO seg- mentation. RPNs completely learn to propose regions from data, and thus can easily beneﬁt from deeper and more expressive features (such as the 101-layer residual nets adopted in [18]). Faster R-CNN and RPN are also used by several other leading entries in these competitions2. These results suggest that our method is not only a cost-efﬁcient solution for practical usage, but also an effective way of improving object detec- tion accuracy.

We comprehensively evaluate our method on the PASCAL VOC detection benchmarks [11] where RPNs with Fast R-CNNs produce detection accuracy bet- ter than the strong baseline of Selective Search with Fast R-CNNs. Meanwhile, our method waives nearly all computational burdens of Selective Search at test-time—the effective running time for proposals is just 10 milliseconds. Using the expensive very deep models of [3], our detection method still has a frame rate of 5fps (including all steps) on a GPU, and thus is a practical object detection system in terms of both speed and accuracy. We also report results on the MS COCO dataset [12] and investi- gate the improvements on PASCAL VOC using the COCO data. Code has been made publicly available at https://github.com/shaoqingren/faster_ rcnn (in MATLAB) and https://github.com/ rbgirshick/py-faster-rcnn (in Python).